---
---
<p style="font-size:18px;">
Due to legal or technical constraints, and the fact that labeling data is expensive and time-consuming, real-world deep-learning pipelines would rarely involve a single fine-tuning stage. Instead, these pipelines could require the sequential fine-tuning of large models in a set of independent tasks that are learned sequentially. Under these conditions, deep neural networks suffer from what is known as catastrophic forgetting, where the fine-tuning on novel tasks significantly reduces the performance of the model in a previously learned task, and drift, where unseen training data does not fit the previously estimated class distribution. Continual learning directly models such a scenario, by adapting a neural network model into a sequential series of tasks. We focus on a special case of CL: class incremental learning (CIL), where the labels and data are mutually exclusive between tasks, training data is available only for the current task, and there are no tasks ids.
  
With 500 hours of video from diverse categories uploaded to YouTube every minute and a billion people actively using TikTok every month, video content of varying quality is available at an unprecedented scale. With such large volumes of data, it is important to develop models that can effectively learn from continuous streams of untrimmed video data. We directly address this problem and propose vCLIMB (video CLass IncreMental Learning Benchmark), a novel benchmark devised for the evaluation of continual learning in video. Our test-bed defines a fixed task split on the original training and validation sets of three well known video datasets: UCF101, ActivityNet and Kinetics. vCLIMB follows the standard class incremental continual learning scenario, but includes some modifications to better fit the nature of video data in human action recognition tasks. First, to achieve fair comparisons between video CIL methods that use memory, we re-define memory size to be the total number of frames, instead of the total number of video instances, that the memory can store. We report this as Memory Frame Capacity in our tables to avoid confusion with the memory size defined in image CIL. This means we are not only concerned about selecting the best videos to store in memory, but we also want to identify the best set of frames to keep in memory. Second, since fine-grained temporal video annotations are expensive (especially for long videos), we analyze the effect of using trimmed and untrimmed video data in continual learning. To the best of our knowledge, this is the first work to explore continual learning with untrimmed videos. Third, We present a novel strategy based on consistency regularization that can be built on top of memory-based methods to reduce memory consumption while improving performance.
</p>
